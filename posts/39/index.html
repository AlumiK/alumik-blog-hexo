<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"pages.alumik.cn","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="One of the most common probability distributions is the normal (or Gaussian) distribution.  Many natural phenomena can be modeled using a normal distribution.  It&#39;s also of great importance due to">
<meta property="og:type" content="article">
<meta property="og:title" content="Sampling from a Normal Distribution">
<meta property="og:url" content="http://pages.alumik.cn/posts/39/">
<meta property="og:site_name" content="AlumiK">
<meta property="og:description" content="One of the most common probability distributions is the normal (or Gaussian) distribution.  Many natural phenomena can be modeled using a normal distribution.  It&#39;s also of great importance due to">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://pages.alumik.cn/posts/39/normal_pdf_cdf.png">
<meta property="og:image" content="http://pages.alumik.cn/posts/39/ks_test.png">
<meta property="og:image" content="http://pages.alumik.cn/posts/39/index_4_2.png">
<meta property="og:image" content="http://pages.alumik.cn/posts/39/index_7_2.png">
<meta property="article:published_time" content="2019-11-16T13:40:19.000Z">
<meta property="article:modified_time" content="2020-02-20T06:03:31.000Z">
<meta property="article:author" content="AlumiK (Zhong Zhenyu)">
<meta property="article:tag" content="概率论与数理统计">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://pages.alumik.cn/posts/39/normal_pdf_cdf.png">

<link rel="canonical" href="http://pages.alumik.cn/posts/39/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Sampling from a Normal Distribution | AlumiK</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?33af24983680ccd6ec7199d8b6f8cd5a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">AlumiK</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">技术笔记</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">31</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">16</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://pages.alumik.cn/posts/39/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://portrait.gitee.com/uploads/avatars/user/1774/5322710_AlumiK_1605847874.png">
      <meta itemprop="name" content="AlumiK (Zhong Zhenyu)">
      <meta itemprop="description" content="~ 记录计算机学习心得的小站 ~">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="AlumiK">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Sampling from a Normal Distribution
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-11-16 21:40:19" itemprop="dateCreated datePublished" datetime="2019-11-16T21:40:19+08:00">2019-11-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-02-20 14:03:31" itemprop="dateModified" datetime="2020-02-20T14:03:31+08:00">2020-02-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学知识</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>One of the most common probability distributions is the normal (or Gaussian) distribution.  Many natural phenomena can be modeled using a normal distribution.  It&#39;s also of great importance due to its relation to the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>.</p>
<p>In this post, we&#39;ll be reviewing the normal distribution and looking at how to draw samples from it using two methods.  The first method using the central limit theorem, and the second method using the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform">Box-Muller transform</a>.  As usual, some brief coverage of the mathematics and code will be included to help drive intuition.</p>
<a id="more"></a>
<h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><h3 id="Normal-Distribution"><a href="#Normal-Distribution" class="headerlink" title="Normal Distribution"></a>Normal Distribution</h3><p>Let&#39;s start off first by covering some basics.  A normal distribution (also known as a Gaussian distribution) \(N \sim (\mu, \sigma)\) has probability density function (PDF) and cumulative density function (CDF) shown here parameterized by its mean (\(\mu\)) and standard deviation (\(\sigma\)) <a href="#fn-1"><sup>[1]</sup></a>:</p>
<script type="math/tex; mode=display">f_N(x) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}} \tag{1}</script><script type="math/tex; mode=display">F_N(x) = \int_{-\infty}^{x}f_N(t) dt  \tag{2}</script><p>The CDF doesn&#39;t have a nice closed form, so we&#39;ll just represent it here using the definition of CDF in terms of its PDF.  We can graph the PDF and CDF (images from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Normal_distribution">Wikipedia</a>) using various values of the two parameters:</p>
<img src="/posts/39/normal_pdf_cdf.png" class="" title="PDF of Normal Distribution">
<p>The normal distribution is sometimes colloquially known as the &quot;bell curve&quot; because of a it&#39;s symmetric hump.  A very common thing to do with a probability distribution is to <em>sample</em> from it.  In other words, we want to randomly generate numbers (i.e. \(x\) values) such that the values of \(x\) are in proportion to the PDF.  So for the standard normal distribution, \(N \sim (0, 1) \) (the red curve in the picture above), most of the values would fall close to somewhere around \(x=0\).  In fact, 68% will fall within \([-1, 1]\), 95% will fall within \([-2,2]\) and 99.7% will fall within \([-3,3]\).  This corresponds to \(\sigma, 2\sigma, 3\sigma\) from the mean, see this <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule">article</a> for more details.</p>
<h3 id="Central-Limit-Theorem"><a href="#Central-Limit-Theorem" class="headerlink" title="Central Limit Theorem"></a>Central Limit Theorem</h3><p>The central limit theorem (CLT) is quite a surprising result relating the sample average of \(n\) <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">independent and identically distributed</a> (i.i.d.) random variables and a normal distribution.  To state it more precisely: </p>
<blockquote>
<p>Let \({X_1, X_2, \ldots, X_n}\) be \(n\) i.i.d. random variables with \(E(X_i)=\mu\)<br>and \(Var(X_i)=\sigma^2\) and  let \(S_n = \frac{X_1 + X_2 + \ldots + X_n}{n}\) be the sample<br>average. Then \(S_n\) approximates a normal distribution with mean of \(\mu\) and<br>variance of \(\frac{\sigma^2}{n}\) for large \(n\) (i.e. \(S_n \approx N(\mu, \frac{\sigma^2}{n})\)).</p>
</blockquote>
<p>The surprising result is that \(X_n\) can be <em>any</em> distribution.  It isn&#39;t restricted to just normal distributions.  We can also define the standard normal distribution in terms of \(S_n\) by shifting and scaling it:</p>
<script type="math/tex; mode=display">N(0,1) \approx \frac{S_n - \mu}{\frac{\sigma}{\sqrt{n}}} = \frac{\sqrt{n}(S_n - \mu)}{\sigma} \tag{3}</script><h3 id="Comparing-Distributions"><a href="#Comparing-Distributions" class="headerlink" title="Comparing Distributions"></a>Comparing Distributions</h3><p>Since our goal is to implement sampling from a normal distribution, it would be nice to know if we actually did it correctly!  One common way to test if two arbitrary distributions are the same is to use the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov–Smirnov test</a>.  In the basic form, we can compare a sample of points with a reference distribution to find their similarity.  </p>
<p>The basic idea of the test is to first sort the points in the sample and the compute the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Empirical_distribution_function">empirical CDF</a>. Next, find the largest absolute difference between any point in the empirical CDF and the theoretical reference distribution.  If the two are the same, this difference should be very small. If it&#39;s large then we can be confident that the distribution is different.  Further, this difference follows a certain distribution, which allows us to test our null hypothesis of whether our samples were drawn from the reference distribution.<br>The following figure (from <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Kolmogorov_distribution">Wikipedia</a>) shows this more clearly:</p>
<img src="/posts/39/ks_test.png" class="" title="Kolmogorov–Smirnov test">
<p>Fortunately, we don&#39;t have to implement this ourselves.  A package is available in <a target="_blank" rel="noopener" href="http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html">scipy.stats</a>.  Let&#39;s play around with it a bit to see how it works.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">123</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># number of samples</span></span><br><span class="line">N=<span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Kolmogorov-Smirnov test Uniform(0,1) vs. reference N(0, 1)</span></span><br><span class="line">samples = stats.uniform.rvs(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=N)</span><br><span class="line">test_stat, pvalue = stats.kstest(samples, <span class="string">&#x27;norm&#x27;</span>, args=(<span class="number">0</span>, <span class="number">1</span>), N=N)</span><br><span class="line">print(<span class="string">&quot;U(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.&quot;</span> % (test_stat, pvalue))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Kolmogorov-Smirnov test N(0, 2) vs. reference N(0, 1)</span></span><br><span class="line">samples = stats.norm.rvs(loc=<span class="number">0</span>, scale=<span class="number">2</span>, size=N) </span><br><span class="line">test_stat, pvalue = stats.kstest(samples, <span class="string">&#x27;norm&#x27;</span>, args=(<span class="number">0</span>, <span class="number">1</span>), N=N)</span><br><span class="line">print(<span class="string">&quot;N(0,2) vs. N(0, 1): KS=%.4f with p-value = %.4f.&quot;</span> % (test_stat, pvalue))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Run Kolmogorov-Smirnov test N(0, 1) vs. reference N(0, 1)</span></span><br><span class="line">samples = stats.norm.rvs(loc=<span class="number">0</span>, scale=<span class="number">1</span>, size=N) </span><br><span class="line">test_stat, pvalue = stats.kstest(samples, <span class="string">&#x27;norm&#x27;</span>, args=(<span class="number">0</span>, <span class="number">1</span>), N=N)</span><br><span class="line">print(<span class="string">&quot;N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.&quot;</span> % (test_stat, pvalue))</span><br></pre></td></tr></table></figure>
<pre><code>U(0,1) vs. N(0, 1): KS=0.5000 with p-value = 0.0000.
N(0,2) vs. N(0, 1): KS=0.1673 with p-value = 0.0000.
N(0,1) vs. N(0, 1): KS=0.0104 with p-value = 0.2288.
</code></pre><p>Using \(N(0,1)\) as our reference distribution, the KS test has a large value and a negligible p-value when comparing to a uniform distribution \(U(0,1)\) (\(KS=0.5\)).  The normal distribution with a wider base \(N(0, 2)\) (\(KS=0.1673\)) also has a negligible p-value.  However, when we compare samples from the identical distribution \(N(0,1)\), we get a relatively small value (\(KS=0.0104\)) for the test statistic and a large p-value, indicating we don&#39;t have sufficient evidence to reject the null hypothesis (that our two distributions are the same).</p>
<h2 id="Sampling-using-the-Central-Limit-Theorem"><a href="#Sampling-using-the-Central-Limit-Theorem" class="headerlink" title="Sampling using the Central Limit Theorem"></a>Sampling using the Central Limit Theorem</h2><p>Now let&#39;s try to use the Central Limit Theorem to sample from \(N(0,1)\).  First let&#39;s define our i.i.d. variable \(X_n\) to have a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a> with \(p=0.5\), which we can intuitively think of tossing an unbiased coin:</p>
<script type="math/tex; mode=display">P(X_n=k) = \begin{cases} p=0.5 & \text{if }k=1, \\[6pt] 1-p = 0.5 & \text {if }k=0.\end{cases} \tag{4}</script><p>Recall, the Bernoulli distribution is closely relates to the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial distribution</a> denoted by \(B(n, p)\) by \(Bernoulli(p) = B(n=1, p)\) .  The Binomial distribution can intuitively be thought of as counting the number of heads in \(n\) tosses of a coin (i.e. Bernoulli trials).  If \(n=1\), it reduces to a Bernoulli distribution (or single coin toss).</p>
<p>Let&#39;s now define our sample average for \(n\) tosses of our unbiased coin:</p>
<script type="math/tex; mode=display">S_n = \frac{X_1 + X_2 + \ldots + X_n}{n} = \frac{B(n, p=0.5)}{n}</script><p>We can see that this distribution has \(\mu=\frac{n}{2}\) (we expect half our tosses to be heads), and \(\sigma^2=\frac{p(1-p)}{n}=\frac{0.25}{n}\) (Bernoulli RVs have \(\sigma^2 = p(1-p)\)).</p>
<p>Shifting and scaling<a href="#fn-2"><sup>[2]</sup></a> this to get our standard normal distribution using Equation 3, we get:</p>
<script type="math/tex; mode=display">\begin{equation}
\begin{aligned} 
N(0,1) &\approx \frac{\sqrt{n}(S_n - \mu)}{\sigma} \\
       &= \frac{\sqrt{n}(\frac{X_1 + X_2 + \ldots + X_n}{n} - 0.5)}{\sqrt{0.25}} \\
       &= 2\sqrt{n}(\frac{X_1 + X_2 + \ldots + X_n}{n} - 0.5)
\end{aligned} \tag{5}
\end{equation}</script><p>Theoretically, this should give us an equation to roughly simulate a standard normal distribution.  Let&#39;s try it!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">N=<span class="number">10000</span></span><br><span class="line">random.seed(<span class="number">123</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bernoulli</span>():</span></span><br><span class="line">    <span class="keyword">return</span> random.randint(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Our sample function of N(0,1) using Equation 5</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampleN_v1</span>(<span class="params">N=<span class="number">2500</span></span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2.0</span> * math.sqrt(N) * (<span class="built_in">sum</span>(bernoulli() <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N)) / N - <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use KS to test again</span></span><br><span class="line">samples = [sampleN_v1() <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">test_stat, pvalue = stats.kstest(samples, <span class="string">&#x27;norm&#x27;</span>, args=(<span class="number">0</span>, <span class="number">1</span>), N=N)</span><br><span class="line">print(<span class="string">&quot;sample_N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.&quot;</span> % (test_stat, pvalue))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Let&#x27;s plot our samples against our reference distribution</span></span><br><span class="line">reference = [stats.norm.rvs() <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;sample_N&#x27;</span>: samples, <span class="string">&#x27;N(0,1)&#x27;</span>: reference&#125;).plot(kind=<span class="string">&#x27;hist&#x27;</span>, bins=<span class="number">100</span>, alpha=<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<pre><code>sample_N(0,1) vs. N(0, 1): KS=0.0114 with p-value = 0.1499.
&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f62c4dd4e80&gt;
</code></pre><img src="/posts/39/index_4_2.png" class="">
<p>Our KS score is somewhat close to \(0\) with a p-value that suggests we can&#39;t reject our null hypothesis.  Graphing our implementation of \(N(0,1)\) with the reference one shows that we have the roughly the right shape.  No doubt by setting larger \(N\) (for both the number of Bernoulli trials and the number of samples drawn), we would get something much closer.  </p>
<p>One large downside of our CLT implementation is that it&#39;s <em>slow</em>.  While the wall clock time of drawing \(10000\) samples using the <code>numpy</code> library is unnoticeable, it takes roughly ten seconds on my machine with the parameters above using our CLT implementation, quite a big difference.  In the next section, we&#39;ll see a much more efficient implementation that uses a &quot;trick&quot; to transform a pair of independent uniform random variables to a pair of independent normal random variables.</p>
<h2 id="Sampling-using-the-Box-Muller-Transform"><a href="#Sampling-using-the-Box-Muller-Transform" class="headerlink" title="Sampling using the Box-Muller Transform"></a>Sampling using the Box-Muller Transform</h2><p>The <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform">Box-Muller transform</a> is a neat little &quot;trick&quot; that allows us to sample from a pair of normally distributed variables using a source of only uniformly distributed variables.  The transform is actually pretty simple to compute.  Given two independent uniformly distributed random variables \(U_1, U_2\) on the interval \((0,1)\), we define two new random variables, \(R\) and \(\Theta\), that intuitively representing <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Polar_coordinate_system">polar coordinates</a> as such:</p>
<script type="math/tex; mode=display">\begin{align}
R    &= \sqrt{-2lnU_1}   \tag{6}\\
\Theta &= 2\pi U_2  \tag{7}
\end{align}</script><p>Now using the standard transformation from polar coordinates \(R, \Theta\) to Cartesian ones \(X, Y\), we claim that \(X\) and \(Y\) are independent standard normally distributed random variables:</p>
<script type="math/tex; mode=display">\begin{align}
X &= Rcos\Theta &= \sqrt{-2lnU_1}cos(2\pi U_2) \tag{8}\\
Y &= Rsin\Theta &= \sqrt{-2lnU_1}sin(2\pi U_2) \tag{9}
\end{align}</script><p>Let&#39;s take a look at the proof to gain some intuition on how this works.</p>
<h3 id="Proof"><a href="#Proof" class="headerlink" title="Proof"></a>Proof</h3><p>Starting with \(U_1, U_2\), let&#39;s see what kind of distributions we have for \(R, \Theta\).  From Equation 7, it should be clear that \(\Theta\) is also uniformly distributed since it&#39;s just multiplying by a constant (\(2\pi\)), but let&#39;s go through the motions to explicitly see that.  From the CDF of \(\Theta\):</p>
<script type="math/tex; mode=display">\begin{equation}
\begin{aligned} 
P(\Theta \leq \theta) &= P(2\pi U_2 \leq \theta) \\
                      &= P(U_2 \leq \frac{\theta}{2\pi}) \\
               \Theta &\sim Uniform(0, 2\pi)
\end{aligned} \tag{10}
\end{equation}</script><p>So we have our first result that \(\Theta\) is uniformly distributed on \((0, 2\pi)\) (as we would expect).  Using a more explicit method, we can find the distribution of \(R\).  From Equation 7, we know that the mapping from \(U_1\) to \(R\) is one-to-one, which we&#39;ll call \(g\) (i.e. \(R=g(U_1)\)).  So if we&#39;re trying to find the probability of R over \((r_1, r_2)\) (by integrating its PDF), there is some equivalent range over \((g^{-1}(r_1), g^{-1}(r_2))\) where we can integrate over the PDF of \(U_1\).  Let&#39;s see how this works <a href="#fn-3"><sup>[3]</sup></a>:</p>
<script type="math/tex; mode=display">\begin{equation}
\begin{aligned} 
P(r_1 \leq R \leq r_2) &=  \int_{g^{-1}(r_2)}^{g^{-1}(r_1)} f_{U_1}(s) ds  && \text{since } g(x) \text{ is strictly decreasing}\\
                       &=  -\int_{g^{-1}(r_1)}^{g^{-1}(r_2)}  ds && \text{since }f_{U_1}(u)=1 \\
                       &=  -\int_{r_1}^{r_2} -t e^{\frac{-t^2}{2}} dt && \text{change variables } t=\sqrt{-2\ln s} \\

\end{aligned} \tag{11}
\end{equation}</script><script type="math/tex; mode=display">\begin{equation}
f_R(r) =  r e^{\frac{-r^2}{2}}  \tag{12}
\end{equation}</script><p>giving us the PDF for \(R\), \(f_R(r)\)<a href="#fn-4"><sup>[4]</sup></a>.  It should also be clear that \(R\) and \(\Theta\) are independent because \(U_1\) and \(U_2\) are independent.</p>
<p>Now that we have the distributions for both \(R\) and \(\Theta\), let&#39;s label Equation 8 and 9 as \(X=g_x(R, \Theta)\) and \(Y=g_y(R, \Theta)\), respectively.  Now we can apply the same procedure as above using variable substitution (for multiple variables) to derive the joint distribution of \(X\) and \(Y\) (we&#39;ll represent the<br>area in the transformed space by \(A\)<a href="#fn-5"><sup>[5]</sup></a>):</p>
<script type="math/tex; mode=display">\begin{equation}
\begin{aligned} 
P(x_1 &\leq X \leq x_2, y_1 \leq Y \leq y_2) \\
    &= \int \int_A f_{R,\Theta}(u, v) du dv \\
    &= \int \int_A f_{R}(u)f_{\Theta}(v) du dv && \text{since } R \text{ and } \Theta \text{ are independent} \\ 
    &= \int \int_A ue^{\frac{-u^2}{2}}\frac{1}{2\pi} du dv
\end{aligned} \tag{13}
\end{equation}</script><p>At this point, we should remember that if \(p=ucos(v), q=usin(v)\), then solving for \(u,v\) results in \(u=\sqrt{q^2+p^2}, v=arctan(\frac{q}{p})\).  Also, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables">substitution for multiple variables</a> means that \(du dv = |det(\frac{\partial(u,v)}{\partial(p, q)})| dp dq\), plugging our expressions for \(u\) and \(v\) in:</p>
<script type="math/tex; mode=display">\begin{equation}
\begin{aligned} 
du dv &= |det(\frac{\partial(u,v)}{\partial(p, q)})| dp dq \\
      &= 
\left |
\begin{array}{cc}
\frac{\partial u}{\partial p} & \frac{\partial u}{\partial q} \\
\frac{\partial v}{\partial p} & \frac{\partial v}{\partial q}
\end{array}
\right | dpdq \\
      &= 
\left |
\begin{array}{cc}
\frac{\partial \sqrt{q^2+p^2}}{\partial p} & \frac{\partial \sqrt{q^2+p^2}}{\partial q} \\
\frac{\partial arctan(\frac{q}{p})}{\partial p} & \frac{\partial arctan(\frac{q}{p})}{\partial q}
\end{array}
\right | dpdq \\
      &= 
\left |
\begin{array}{cc}
\frac{p}{\sqrt{p^2+q^2}} & \frac{q}{\sqrt{p^2+q^2}} \\
\frac{-q}{p^2(1+(\frac{q}{p})^2)} & \frac{1}{p(1+(\frac{q}{p})^2)}
\end{array}
\right | dpdq \\
      &= \frac{1}{\sqrt{p^2 + q^2}} dpdq
\end{aligned} \tag{14}
\end{equation}</script><p>Using the result from Equation 14 into Equation 13, we get:</p>
<script type="math/tex; mode=display">\begin{equation}
\begin{aligned}
P(&x_1 \leq X \leq x_2, y_1 \leq Y \leq y_2) && \text{change variables} \\
    &= \int_{x_1}^{x_2} \int_{y_1}^{y_2} \frac{\sqrt{p^2 + q^2}}{2\pi}\cdot e^{\frac{-(p^2 + q^2)}{2}} \cdot \frac{1}{\sqrt{p^2 + q^2}} dp dq  && p=ucos(v)  \\
    &                     && q=usin(v) \\
    &= \int_{x_1}^{x_2} \underbrace{\frac{1}{\sqrt{2\pi}} e^{\frac{-p^2}{2}}}_{f_X(x)} dp \int_{y_1}^{y_2} \underbrace{\frac{1}{\sqrt{2\pi}} e^{\frac{-q^2}{2}}}_{f_Y(y)} dq
\end{aligned} \tag{15}
\end{equation}</script><p>Equation 15 shows that \(X\) and \(Y\) are independent each with PDF matching our standard normal distribution \(N(0,1)\) as required.</p>
<h3 id="Implementing-Box-Muller-Transform"><a href="#Implementing-Box-Muller-Transform" class="headerlink" title="Implementing Box-Muller Transform"></a>Implementing Box-Muller Transform</h3><p>The implementation is a relatively straight forward application of Equation 8 and 9:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">N=<span class="number">10000</span></span><br><span class="line">random.seed(<span class="number">123</span>)</span><br><span class="line">epsilon = sys.float_info.epsilon</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">box_muller</span>():</span></span><br><span class="line">    <span class="comment"># Avoid getting u == 0.0</span></span><br><span class="line">    u1, u2 = <span class="number">0.0</span>, <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">while</span> u1 &lt; epsilon <span class="keyword">or</span> u2 &lt; epsilon:</span><br><span class="line">        u1 = random.random()</span><br><span class="line">        u2 = random.random()</span><br><span class="line">        </span><br><span class="line">    n1 = math.sqrt(<span class="number">-2</span> * math.log(u1)) * math.cos(<span class="number">2</span> * math.pi * u2)</span><br><span class="line">    n2 = math.sqrt(<span class="number">-2</span> * math.log(u1)) * math.sin(<span class="number">2</span> * math.pi * u2)</span><br><span class="line">    <span class="keyword">return</span> n1, n2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use KS to test</span></span><br><span class="line">samples = [box_muller()[<span class="number">0</span>] <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">test_stat, pvalue = stats.kstest(samples, <span class="string">&#x27;norm&#x27;</span>, args=(<span class="number">0</span>, <span class="number">1</span>), N=N)</span><br><span class="line">print(<span class="string">&quot;sample_N(0,1) vs. N(0, 1): KS=%.4f with p-value = %.4f.&quot;</span> % (test_stat, pvalue))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot our samples against our reference distribution</span></span><br><span class="line">reference = [stats.norm.rvs() <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(N)]</span><br><span class="line">pd.DataFrame(&#123;<span class="string">&#x27;sample_N&#x27;</span>: samples, <span class="string">&#x27;N(0,1)&#x27;</span>: reference&#125;).plot(kind=<span class="string">&#x27;hist&#x27;</span>, bins=<span class="number">100</span>, alpha=<span class="number">0.4</span>)</span><br></pre></td></tr></table></figure>
<pre><code>sample_N(0,1) vs. N(0, 1): KS=0.0074 with p-value = 0.6372.

&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f62c509fcc0&gt;
</code></pre><img src="/posts/39/index_7_2.png" class="">
<p>As we can see, our Box-Muller method of sampling from \( N(0,1) \) generates quite good results.  Our KS test statistic is quite small along with a large p-value (so we can&#39;t reject our null hypothesis).  Similarly, the graph shows the expected shape matching our reference distribution.  The one big advantage this method has though is that it&#39;s quite fast.  There&#39;s no noticeable lag when generating (N=10000) samples.</p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Generating pseudo-random numbers according to various probability distributions has many applications, for example, in Markov Chain Monte Carlo (MCMC) techniques.  In this post, we looked at one of the most fundamental distributions: the standard normal distribution.  Our first method uses a result directly from the central limit theorem which results in an inefficient implementation.  However with a bit of grit and calculus, we were able to show that the Box-Muller transform provides a much more elegant solution to sampling from a standard normal distribution leading us to an efficient implementation.  Hopefully this sheds some light on how to sample a normal distribution.</p>
<h2 id="References-and-Further-Reading"><a href="#References-and-Further-Reading" class="headerlink" title="References and Further Reading"></a>References and Further Reading</h2><ul>
<li>Wikipedia: <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Normal_distribution">Normal Distribution</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform">Box-Muller Transform</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central Limit Theorem</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Integration_by_substitution#Substitution_for_multiple_variables">Substitution for multiple variables</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Determinant#2.C2.A0.C3.97.C2.A02_matrices">Determinant of 2x2 matrices</a>, <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian matrix and determinant</a></li>
<li>Stack Exchange: <a target="_blank" rel="noopener" href="http://stats.stackexchange.com/questions/16334/how-to-sample-from-a-normal-distribution-with-known-mean-and-variance-using-a-co">How to sample from a normal distribution with known mean and variance using a conventional programming language?</a>, <a target="_blank" rel="noopener" href="http://math.stackexchange.com/questions/1110168/proof-of-the-box-muller-method">Proof of Box-Muller method</a></li>
<li><a target="_blank" rel="noopener" href="http://www.math.uah.edu/stat/dist/Transformations.html">Transformations of Random Variables</a> (University of Alabama Huntsville)</li>
<li><a target="_blank" rel="noopener" href="http://tutorial.math.lamar.edu/Classes/CalcIII/ChangeOfVariables.aspx">Change of Variables</a> (Paul&#39;s Online Math Notes)</li>
<li><a target="_blank" rel="noopener" href="http://www.math.nyu.edu/faculty/goodman/teaching/MonteCarlo2005/notes/GaussianSampling.pdf">Simple Sampling of Gaussians</a> (Jonathan Goodman, NYU)</li>
</ul>
<h2 id="Notes"><a href="#Notes" class="headerlink" title="Notes"></a>Notes</h2><p>List of Notes: <sup><a href="#fn_1" id="reffn_1">1</a></sup>, <sup><a href="#fn_2" id="reffn_2">2</a></sup>, <sup><a href="#fn_3" id="reffn_3">3</a></sup>, <sup><a href="#fn_4" id="reffn_4">4</a></sup>, <sup><a href="#fn_5" id="reffn_5">5</a></sup></p>
<blockquote id="fn_1">
<sup>1</sup>. We&#39;ll use the convention of \(f_X(x)\) and \(F_X(x)\) to denote the PDF and CDF of random variable X, respectively.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_2">
<sup>2</sup>. If we got back to the definitions of mean and variance, we can see this shifting and scaling yields the correct result.  \(E(\frac{\sqrt{n}(S_n - \mu)}{\sigma}) = \frac{\sqrt{n}(E(S_n) - \mu)}{\sigma} = 0\) since \(E(S_n)=\mu\) by definition.  And \(Var(\frac{\sqrt{n}(S_n - \mu)}{\sigma}) = \frac{n}{\sigma^2}(E((S_n - \mu)^2) - (E(S_n - \mu))^2 = \frac{n}{\sigma^2} \frac{\sigma^2}{n} = 1 \)<a href="#reffn_2" title="Jump back to footnote [2] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. We use \(s, t\) for the dummy variables of \(U_1, R\) respectively.  You might also need a refresher on <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Integration_by_substitution">integration by substitution</a> like I did.  Also notice that we flipped the integral endpoints of \(g^{-1}(r1)\) and \(g^{-1}(r2)\) because \(g(x)\) is a strictly decreasing function, so the lower limit \(r_1\) maps to the upper limit \(g^{-1}(r_1)\) in the transformed function.  Similarly with \(r_2\) and \(g^{-1}(r_2)\).<a href="#reffn_3" title="Jump back to footnote [3] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. Our result is actually a specific case of a more general result when transforming from one probability distribution.  Take a look at <a target="_blank" rel="noopener" href="http://www.math.uah.edu/stat/dist/Transformations.html">Transformations of Random Variables</a> for more details and examples.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> &#8617;</a>
</blockquote>
<blockquote id="fn_5">
<sup>5</sup>. Remember that the rectangular area represented by \((x_1, x_2)\) and \((y_1, y_2)\) is non-rectagular when we transform it into the \(R\) and \(\Theta\) space.  This means that we&#39;re no longer integrating just along \(R\) and \(\Theta\) separately but together along the new oddly shaped area, thus the need to use \(A\) to represent the area.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> &#8617;</a>
</blockquote>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><ul>
<li><a target="_blank" rel="noopener" href="http://bjlkeng.github.io/posts/sampling-from-a-normal-distribution/">http://bjlkeng.github.io/posts/sampling-from-a-normal-distribution/</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/statistics/" rel="tag"><i class="fa fa-tag"></i> 概率论与数理统计</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/38/" rel="prev" title="Mapping MySQL Data Types in Java">
      <i class="fa fa-chevron-left"></i> Mapping MySQL Data Types in Java
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/40/" rel="next" title="在 Matplotlib 中显示中文">
      在 Matplotlib 中显示中文 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Background"><span class="nav-number">1.</span> <span class="nav-text">Background</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-Distribution"><span class="nav-number">1.1.</span> <span class="nav-text">Normal Distribution</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Central-Limit-Theorem"><span class="nav-number">1.2.</span> <span class="nav-text">Central Limit Theorem</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Comparing-Distributions"><span class="nav-number">1.3.</span> <span class="nav-text">Comparing Distributions</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sampling-using-the-Central-Limit-Theorem"><span class="nav-number">2.</span> <span class="nav-text">Sampling using the Central Limit Theorem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sampling-using-the-Box-Muller-Transform"><span class="nav-number">3.</span> <span class="nav-text">Sampling using the Box-Muller Transform</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Proof"><span class="nav-number">3.1.</span> <span class="nav-text">Proof</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Implementing-Box-Muller-Transform"><span class="nav-number">3.2.</span> <span class="nav-text">Implementing Box-Muller Transform</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">4.</span> <span class="nav-text">Conclusion</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References-and-Further-Reading"><span class="nav-number">5.</span> <span class="nav-text">References and Further Reading</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Notes"><span class="nav-number">6.</span> <span class="nav-text">Notes</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E9%93%BE%E6%8E%A5"><span class="nav-number">7.</span> <span class="nav-text">参考链接</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="AlumiK (Zhong Zhenyu)"
      src="https://portrait.gitee.com/uploads/avatars/user/1774/5322710_AlumiK_1605847874.png">
  <p class="site-author-name" itemprop="name">AlumiK (Zhong Zhenyu)</p>
  <div class="site-description" itemprop="description">~ 记录计算机学习心得的小站 ~</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">57</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">31</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/AlumiK" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;AlumiK" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:nczzy1997@gmail.com" title="E-Mail → mailto:nczzy1997@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">AlumiK (Zhong Zhenyu)</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
